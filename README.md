## Overview
This repository provides the codes to reproduce the results from *Jianping Ye, Michel Wedel, Pieters, AdGazer: Improving Contextual Advertising with Theory-Informed Machine Learning*. We propose an *Alternative Disclosure Plan* for a delay of the release of our main dataset of the ad-context pairs. However, to maximize the reproducibility of the paper's results in this situation, we provide:

- All preprocessed data used in the paper for model training, i.e. all features extracted from ad and context images in our main dataset with our algorithms;
- All codes for the algorithms and models used in the paper;
- All trained models (Sentence Transformer, XGBoost, CNN) and pre-trained models used;
- Complete codes and data that reproduce the results for the study of Out-of-Distribution generalization, the study of the model interpretation and the study of the controlled experiment on ad placement;
- Complete codes and partial data that illustrate how we conduct the study of the In-Silico experiments;
- Complete codes for deploying the web app.

## Computational Requirements
The computation environment is specified by the YAML `environment.yml`. Run the following to set up your environment with conda:
```bash
$ conda env create -f environment.yml
```

For all codes in this repository that reproduce the results in the paper, an Intel CPU on [Colab (an online coding platform provided by Google)](https://colab.google) is sufficient. For codes of Multimodal Large Language Models (MLLM) and Convolutional Neural Networks (CNNs), an NVIDIA T4 GPU with 15GB provided by Colab is sufficient.

## Folder and File Descriptions
### Source Files in `src`
This section describes the source files in the `src` folder:
- `Ad_Gaze_Model`: the folder containing parameters of the 10 XGBoost models for ad gaze predictions.
- `Brand_Gaze_Model`: the folder containing parameters of the 10 XGBoost models for brand gaze predictions.
- `Brand_Share_Model`: the folder containing parameters of the 10 XGBoost models for brand share predictions.
- `CNN_Gaze_Model`: the folder containing parameters of the CNN model fine-tuned by ad gaze (AG), brand gaze (BG) and brand share (BS).
- `EAST-Text-Detection`: the folder containing parameters of the EAST text detection model.
- `Magazine_Topic_Embedding_sample_size15`: the folder containing parameters and configurations of the sentence transformer for topic embeddings.
- `SIFT`: 
    - `kmeans.pt`: the pytorch file containing the k-mean cluster info used by our SIFT feature extractor.
    - `pca.pt`: the pytorch file containing the pca info used to compress the SIFT features.
- `Topic_Embedding_PCAs`: the 10 PCA info used to compress the topic embeddings. The ith PCA info was calculated from the ith-fold training data.
- `DL_models.py`: the python file containing codes of all deep learning models used in the paper.
- `Predict.py`: the python file containing codes of the attention predictions.
- `XGBoost_utils.py`: the python file containing codes of all the algorithms and utility functions used.


### Model Parameters of the Trained Sentence Transformer Model
Download our [trained model](https://drive.google.com/file/d/1_Vv1AXZsQGw41s-Q3bcg-k6aos5fK0Pd/view?usp=sharing). The downloaded file should be put under `src/Magazine_Topic_Embedding_sample_size15`.


### Study: Out-of-Distribution (OOD) Generalization (folder: Out-of-Distribution_Samples)
Run `main.py` to reproduce the OOD generalization results in the paper. 

Other folders contain the ad and context images of each OOD ad category, and the essential features used for predictions and evaluations. For example, `Banner` folder contains all 11 banner ads (labeled by the numbers), and also contains (all saved as pytorch file):
- `AGs`/`BGs`: ground-truth ad gaze and brand gaze of the banner ads.
- `Banner_ad_topic_embeddings`: all topic embeddings of the banner ads.
- `Banner_ctpg_topic_embeddings`: all topic embeddings of the contexts of the banner ads.
- `Banner_Adversarial_Categories`: all ad categories of the banner ads.
- `Banner_Adversarial_Media_Categories`: all media categories of the banner ads.
- `Banner_Adversarial_Surfaces`: all element sizes of the banner ads.


### Study: Model Interpretation (folder: Shapley_and_ALE_Values)
Run the code cells in `Shapley_ALE.ipynb` to reproduce the results of Shapley values and ALE values.
- `Data`: the folder containing all the preprocessed data of the images of the main dataset.
- `Figures`: the destination folder that will contain the figures generated by `Shapley_ALE.ipynb`.
- `ALE_interpretation_simple_grid_abs_of_mean_avg_on_x_centered_3200_ads`: all ALE values.
- `ft_dict`: the dictonary of all feature names and their indices.
- `samples`: the pre-defined samples used to calculate interaction effects.


### Study: Controlled Experiment on Ad Placement (folder: Controlled_Experiment_on_Ad_Placement)
Run the R script in `R-script_BANOVA_Experiment.txt` to reproduce the results on this randomized experiment.

The folder also includes:
- `raw data`: the folder that contains all the ads (`ads`), the editorials (`eds`) and the table of recorded gaze and ad element sizes (`randomized_experiment_data.sav`).
- `data_exp.csv`: the csv file that contains processed ad/brand gaze data given random/observed/no context after matching process. Details about the preprocessing can be found in Section *Controlled Experiment on Ad Placement* of the paper.


### Study: In-Sillico Experiments on Optimal Ad Placement (folder: In-Silico_Experiments_random)
Run the code cells in `Experiment_Notebook.ipynb` to understand how the ad placement optimization is done.

**Note:** Currently, we only share 50 sample ads and their contexts, along with their ground-truth ad and brand gaze, randomly selected from our main dataset to illustrate how the optimization works. These 50 samples are in the subfolder `Sample_Ad_Data/stimuli`, and their ad/brand gaze are in `true_AGs.pt`/`true_BGs.pt` (which should be loaded by `torch.load()`).

The folder also includes:
- `Pseudo-Magazines`: the folder containing all pseudo magazine features generated from the randomly selected 50 sample ads.
- `Text_Only_Vars`: the foler containing all the essential models and PCA info to implement the text-only predictions and placements.
- `average_high_level.pt`: the pytorch file containing the averages of the high-level features that help turn off the individual ones for low-level feature based ad placement.
- `average_nonobserv.pt`: the pytorch file containing the averages of the non-observable features that help turn off the individual ones for observable feature based ad placement.
- `Magazine_Optimization.py`: the python file containing all codes for the optimization algorithms.

### Web App
The web app AdGazer can be initiated by running `AdGazer_WebApp.py`.

**Note:** If GPUs are available locally, for full capability of the AdGazer, you may uncomment the image caption generation code, lines 108-114, and comment out lines 115-117. By default, the image caption generation function is turned off due to potential computation limit.


## Contact
If you have any questions on running the codes, please contact jpye00@umd.edu.