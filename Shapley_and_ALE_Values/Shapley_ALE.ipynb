{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "folder_path = os.path.join(parent_dir, \"src\")\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interventional_group_interaction(model, X, group_indices):\n",
    "    #group_indices: currently only support two groups\n",
    "    D = X.shape[0]\n",
    "    pred = model.predict(X)\n",
    "    avg = np.mean(pred)\n",
    "    group_interaction = np.zeros(D)\n",
    "\n",
    "    for i in range(D):\n",
    "        temp_col1 = np.tile(X[:,group_indices[0]][i],(D,1))\n",
    "        temp_col2 = np.tile(X[:,group_indices[1]][i],(D,1))\n",
    "        X1 = X.copy()\n",
    "        X1[:,group_indices[0]] = temp_col1\n",
    "        X2 = X.copy()\n",
    "        X2[:,group_indices[1]] = temp_col2\n",
    "\n",
    "        group_interaction[i] = 1/D*(D*pred[i]-np.sum(model.predict(X1))-np.sum(model.predict(X2))+D*avg)\n",
    "\n",
    "    return group_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dict = torch.load('ft_dict') #The dictionary of feature names and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Shapley values\n",
    "Gaze_Types = ['Ad_Gaze', 'Brand_Gaze', 'Brand_Share']\n",
    "shap_values_tot = []\n",
    "predicted_values_tot = []\n",
    "for gtype in Gaze_Types:\n",
    "    shap_values = 0\n",
    "    predict_vals = 0\n",
    "    for i in range(10):\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        model_path = os.path.join(parent_dir, f'src/{gtype}_Model/10_models/Model_{i+1}.json')\n",
    "        xgb_model.load_model(model_path)\n",
    "        (X_train, Y_train, X_test, Y_test) = torch.load('Data/dataset_'+str(i))\n",
    "        dtrain = xgb.DMatrix(X_train,label=Y_train.reshape(-1))\n",
    "        explainer = shap.TreeExplainer(xgb_model)\n",
    "        shap_values += explainer.shap_values(np.array(np.concatenate((X_train,X_test),axis=0)))/10\n",
    "        predict_vals += xgb_model.predict(np.array(np.concatenate((X_train,X_test),axis=0)))/10\n",
    "    shap_values_tot.append(shap_values)\n",
    "    predicted_values_tot.append(predict_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_samples = torch.load('samples') #samples for calculting interaction effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate interaction effects\n",
    "interaction_shap_tot_ad_ctx = []\n",
    "interaction_shap_tot_hi_low = []\n",
    "Ad_ind = np.array([0,1,2,3,4,6,7,8,12,13,14,18,20,22]+list(range(24,31))+[38]+list(range(40,45))+list(range(50,53))+list(range(56,65))\n",
    "                  +[65,66]+list(range(67,109))+[110])\n",
    "Ctpg_ind = np.array([5,9,10,11,15,16,17,19,21,23]+list(range(31,38))+[39]+list(range(45,50))+list(range(53,56))+[109])\n",
    "groups_ad_ctx = [Ad_ind,Ctpg_ind]\n",
    "\n",
    "High_ind = np.array([20,21,22,23,50,51,52,53,54,55,56]+list(range(56,65))+list(range(67,110)))\n",
    "Low_ind = np.array(list(range(0,20))+list(range(24,50))+[65,66])\n",
    "groups_hi_low = [High_ind, Low_ind]\n",
    "\n",
    "for gtype in Gaze_Types:\n",
    "    I_Ad_Ctpg = 0\n",
    "    I_Hi_Low = 0\n",
    "    for i in range(10):\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        model_path = os.path.join(parent_dir, f'src/{gtype}_Model/10_models/Model_{i+1}.json')\n",
    "        xgb_model.load_model(model_path)\n",
    "        (X_train, Y_train, X_test, Y_test) = torch.load('Data/dataset_'+str(0))\n",
    "        I_Ad_Ctpg += interventional_group_interaction(xgb_model, np.concatenate((X_train,X_test),axis=0)[interaction_samples], groups_ad_ctx)/10\n",
    "        I_Hi_Low += interventional_group_interaction(xgb_model, np.concatenate((X_train,X_test),axis=0)[interaction_samples], groups_hi_low)/10\n",
    "    interaction_shap_tot_ad_ctx.append(I_Ad_Ctpg)\n",
    "    interaction_shap_tot_hi_low.append(I_Hi_Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load pre-calculated ALE values\n",
    "ALE_values_tot_continuous = torch.load('ALE_interpretation_simple_grid_abs_of_mean_avg_on_x_centered_3200_ads.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def Shapley_Interpretation(shap_values_tot, ale_values_tot, \n",
    "                           groups, group_names, \n",
    "                           bar_colors, \n",
    "                           fig_name, fig_title, fig_xlabel, figsize,\n",
    "                           xlim=100, legend_labels=None, legend_loc='lower right',\n",
    "                           edgecolor=None,\n",
    "                           sort=True, groupwise_percent=None,\n",
    "                           ylabel_fontsize=20,\n",
    "                           yticks_fonsize=18,\n",
    "                           legend_fontsize=15,\n",
    "                           x_tick_fontsize=12,\n",
    "                           print_shapley=None,\n",
    "                           supxlabel_y=-0.01,\n",
    "                           height=0.4, bar_gap=0, yaxis_gap=1):\n",
    "    Gaze_Types = ['Ad Gaze', 'Brand Gaze', 'Brand Gaze Share']\n",
    "    fig,ax = plt.subplots(1,len(Gaze_Types), sharey=True, sharex=True, figsize=figsize)\n",
    "    fig.suptitle(fig_title, fontsize=yticks_fonsize)\n",
    "    for i in range(len(Gaze_Types)):\n",
    "        Vertical = []\n",
    "        Horizontal = []\n",
    "        shapley_groups = {}\n",
    "        for j in range(len(group_names)):\n",
    "            curr_group = groups[j]\n",
    "            shap_vals = np.sum(np.abs(shap_values_tot[i][:,curr_group]))\n",
    "            Vertical.append(group_names[j])\n",
    "            Horizontal.append(shap_vals)\n",
    "            shapley_groups[group_names[j]] = shap_vals\n",
    "        if print_shapley:\n",
    "            print('Horizontal: ', Horizontal, 'Vertical: ', Vertical)\n",
    "            if groupwise_percent is not None:\n",
    "                print('High: ', np.argsort(-np.array(Horizontal[:groupwise_percent])))\n",
    "                print('Low: ', np.argsort(-np.array(Horizontal[groupwise_percent:])))\n",
    "\n",
    "        if ale_values_tot is None:\n",
    "            Y_axis = np.arange(len(group_names)-1,-1,-1)\n",
    "            ax[i].barh(Y_axis, np.round(Horizontal,3), align='center', color=bar_colors)\n",
    "            ax[i].bar_label(ax[i].containers[0],label_type='edge', fontsize=ylabel_fontsize)\n",
    "            ax[i].set_yticks(Y_axis, group_names, fontsize=yticks_fonsize)\n",
    "            ax[i].set_xlim([0,xlim])\n",
    "            ax[i].tick_params(axis='x', labelsize=x_tick_fontsize)\n",
    "            ax[i].set_title(Gaze_Types[i], fontsize=yticks_fonsize)\n",
    "            continue\n",
    "\n",
    "        #ALE interpretation\n",
    "        ale_groups = {}\n",
    "        Vertical_ale = []\n",
    "        Horizontal_ale = []\n",
    "        for j in range(len(group_names)):\n",
    "            curr_group = groups[j]\n",
    "            ale_vals = np.sum(np.abs(ale_values_tot[i][:,curr_group]))\n",
    "            Vertical_ale.append(group_names[j])\n",
    "            Horizontal_ale.append(ale_vals)\n",
    "            ale_groups[group_names[j]] = ale_vals\n",
    "        if print_shapley:\n",
    "            print('Horizontal ALE: ', Horizontal_ale, 'Vertical_ALE: ', Vertical_ale)\n",
    "            if groupwise_percent is not None:\n",
    "                print('High ale: ', np.argsort(-np.array(Horizontal_ale[:groupwise_percent])))\n",
    "                print('Low ale: ', np.argsort(-np.array(Horizontal_ale[groupwise_percent:])))\n",
    "\n",
    "        if groupwise_percent is not None:\n",
    "            Y_axis = np.arange((len(group_names)-1)*yaxis_gap,-1*yaxis_gap,-1*yaxis_gap)\n",
    "            rects1 = ax[i].barh(Y_axis[:groupwise_percent]+height/2+bar_gap, np.round(Horizontal[:groupwise_percent]/np.sum(Horizontal[:groupwise_percent])*100,1), height=height, align='center', color=bar_colors[0], label='Shapley')\n",
    "            rects2 = ax[i].barh(Y_axis[:groupwise_percent]-height/2-bar_gap, np.round(Horizontal_ale[:groupwise_percent]/np.sum(Horizontal_ale[:groupwise_percent])*100,1), height=height, align='center', color=bar_colors[0], hatch='///', edgecolor='white',label='ALE')\n",
    "            rects3 = ax[i].barh(Y_axis[groupwise_percent:]+height/2+bar_gap, np.round(Horizontal[groupwise_percent:]/np.sum(Horizontal[groupwise_percent:])*100,1), height=height, align='center', color=bar_colors[1], label='Shapley')\n",
    "            rects4 = ax[i].barh(Y_axis[groupwise_percent:]-height/2-bar_gap, np.round(Horizontal_ale[groupwise_percent:]/np.sum(Horizontal_ale[groupwise_percent:])*100,1), height=height, align='center', color=bar_colors[1], hatch='///', edgecolor='white',label='ALE')\n",
    "        else:\n",
    "            Y_axis = np.arange((len(group_names)-1)*yaxis_gap,-1*yaxis_gap,-1*yaxis_gap)\n",
    "            ax[i].barh(Y_axis+height/2+bar_gap, np.round(Horizontal/np.sum(Horizontal)*100,1), height=height, align='center', color=bar_colors, edgecolor=edgecolor, label='Shapley')\n",
    "            ax[i].barh(Y_axis-height/2-bar_gap, np.round(Horizontal_ale/np.sum(Horizontal_ale)*100,1), height=height, align='center', color=bar_colors, hatch='///', edgecolor='white', label='ALE')\n",
    "        ax[i].bar_label(ax[i].containers[0], label_type='edge', fontsize=ylabel_fontsize)\n",
    "        ax[i].bar_label(ax[i].containers[1], label_type='edge', fontsize=ylabel_fontsize)\n",
    "        if groupwise_percent is not None:\n",
    "            ax[i].bar_label(ax[i].containers[2], label_type='edge', fontsize=ylabel_fontsize)\n",
    "            ax[i].bar_label(ax[i].containers[3], label_type='edge', fontsize=ylabel_fontsize)\n",
    "        ax[i].set_yticks(Y_axis, group_names, fontsize=yticks_fonsize)\n",
    "        ax[i].set_xlim([0,xlim])\n",
    "        ax[i].tick_params(axis='x', labelsize=x_tick_fontsize)\n",
    "        ax[i].set_title(Gaze_Types[i], fontsize=yticks_fonsize)\n",
    "    \n",
    "    if legend_labels is not None:\n",
    "        legend_handles = [\n",
    "            mpatches.Patch(color='tomato', label='High-level'),\n",
    "            mpatches.Patch(color='limegreen', label='Low-level'),\n",
    "            mpatches.Patch(facecolor='white', edgecolor='black', hatch='', label='Shapley'),\n",
    "            mpatches.Patch(facecolor='white', edgecolor='black', hatch='///', label='ALE'),\n",
    "        ]\n",
    "        ax[i].legend(handles=legend_handles, fontsize=legend_fontsize, loc='lower right')\n",
    "        # ax[i].legend((rects1[0], rects3[0]), legend_labels, loc='lower right', fontsize=legend_fontsize)\n",
    "    else:\n",
    "        legend_handles = [\n",
    "            mpatches.Patch(facecolor='white', edgecolor='black', hatch='', label='Shapley'),\n",
    "            mpatches.Patch(facecolor='white', edgecolor='black', hatch='///', label='ALE'),\n",
    "        ]\n",
    "        ax[i].legend(handles=legend_handles, fontsize=legend_fontsize)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_name)\n",
    "    fig.supxlabel(fig_xlabel, fontsize=yticks_fonsize, x=0.5, y=supxlabel_y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ad v.s. Ctpg\n",
    "Ad_ind = np.array([0,1,2,3,4,6,7,8,12,13,14,18,20,22]+list(range(24,31))+[38]+list(range(40,45))+list(range(50,53))+list(range(67,109))+[110])\n",
    "Ctpg_ind = np.array([5,9,10,11,15,16,17,19,21,23]+list(range(31,38))+[39]+list(range(45,50))+list(range(53,56))+list(range(56,65))\n",
    "                +[65,66]+[109])\n",
    "groups = [Ad_ind,Ctpg_ind]\n",
    "group_names = ['Ad', 'Context']\n",
    "bar_colors = ['skyblue', 'orange','white']\n",
    "fig_name = 'Figures/Ad_Ctpg'\n",
    "fig_title = 'Ad and Context Effects'\n",
    "fig_xlabel = 'Percentage of the Total Absolute Shapley/ALE Values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shapley_Interpretation(shap_values_tot, ALE_values_tot_continuous,\n",
    "                       groups, group_names, \n",
    "                       bar_colors, \n",
    "                       fig_name, fig_title, fig_xlabel, figsize=(20,5), sort=False,\n",
    "                       x_tick_fontsize=15,\n",
    "                       supxlabel_y=-0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low v.s. High\n",
    "High_ind = np.array([20,21,22,23,50,51,52,53,54,55]+list(range(56,65))+list(range(67,111)))\n",
    "Low_ind = np.array(list(range(0,20))+list(range(24,50))+[65,66])\n",
    "groups = [High_ind, Low_ind]\n",
    "group_names = ['High-Level','Low-Level']\n",
    "bar_colors = ['tomato', 'limegreen']\n",
    "fig_name = 'Figures/High_Low'\n",
    "fig_title = 'High-level and Low-level Effects'\n",
    "fig_xlabel = 'Percentage of the Total Absolute Shapley/ALE Values'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shapley_Interpretation(shap_values_tot, ALE_values_tot_continuous,\n",
    "                       groups, group_names, \n",
    "                       bar_colors, \n",
    "                       fig_name, fig_title, fig_xlabel, figsize=(20,5), \n",
    "                       legend_loc='upper right', sort=False,\n",
    "                       x_tick_fontsize=15,\n",
    "                       supxlabel_y=-0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Granular\n",
    "group_names = ['Ad Typicality',\n",
    "               'Product & Media Categories',\n",
    "        'Ad Aesthetics',\n",
    "        'Context Aesthetics',\n",
    "        'Ad Copy',\n",
    "        'Editorial Text',\n",
    "        'Ad Objects',\n",
    "        'Context Objects',\n",
    "        'Ad Topics & Contextual Fit',\n",
    "\n",
    "        'Ad Complexity',\n",
    "        'Context Complexity',\n",
    "        'Ad Salience',\n",
    "        'Context Salience',\n",
    "        'Ad Texture',\n",
    "        'Context Texture',\n",
    "        'Ad Element Sizes',\n",
    "        'Ad Size',\n",
    "        'Location']\n",
    "groups = [np.array([110]),\n",
    "          np.array(list(range(56,65))+list(range(71,109))),  \n",
    "       np.array(list(range(50,53))),\n",
    "       np.array(list(range(53,56))),\n",
    "       np.array([20]),\n",
    "       np.array([21]),\n",
    "       np.array([22]),\n",
    "       np.array([23]),\n",
    "       np.array(list(range(67,71))+[109]),\n",
    "       \n",
    "       np.array([4]),\n",
    "       np.array([5]),\n",
    "       np.array([6,7,8,12,13,14,18]),\n",
    "       np.array([9,10,11,15,16,17,19]),\n",
    "       np.array(list(range(24,31))+[38,40,41,42,43,44]),\n",
    "       np.array(list(range(31,38))+[39,45,46,47,48,49]),\n",
    "       np.array([0,1,2]),\n",
    "       np.array([3]),\n",
    "       np.array([24,25])]\n",
    "bar_colors = ['tomato', 'limegreen']\n",
    "fig_name = 'Figures/Group'\n",
    "fig_title = 'Granular Group Effects'\n",
    "fig_xlabel = 'Percentage of the Total Absolute Shapley/ALE Values'\n",
    "legend_labels = ['Low-Level', 'High-Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shapley_Interpretation(shap_values_tot, ALE_values_tot_continuous,\n",
    "                       groups, group_names, \n",
    "                       bar_colors, fig_name, fig_title, fig_xlabel, figsize=(26,13), \n",
    "                       xlim=75, legend_labels=legend_labels,\n",
    "                       sort=False, groupwise_percent=9,\n",
    "                       ylabel_fontsize=14,\n",
    "                       yticks_fonsize=22,\n",
    "                       legend_fontsize=16,\n",
    "                       x_tick_fontsize=15,\n",
    "                       supxlabel_y=-0.03,\n",
    "                       height=5, bar_gap=0, yaxis_gap=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALE Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALE\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def empirical_cdf(sample):\n",
    "    \"\"\"Compute the empirical CDF of a sample.\"\"\"\n",
    "    sorted_sample = np.sort(sample)\n",
    "    n = len(sample)\n",
    "    return sorted_sample, np.arange(1, n + 1) / n\n",
    "\n",
    "def KS_dist(sample1, sample2):\n",
    "    \"\"\"Compute the Kolmogorov-Smirnov distance between two samples.\"\"\"\n",
    "    x1, y1 = empirical_cdf(sample1)\n",
    "    x2, y2 = empirical_cdf(sample2)\n",
    "    \n",
    "    # Combine the unique points from both samples\n",
    "    all_x = np.sort(np.concatenate((x1, x2)))\n",
    "    \n",
    "    # Compute ECDF values at these points\n",
    "    ecdf1 = np.searchsorted(x1, all_x, side='right') / len(sample1)\n",
    "    ecdf2 = np.searchsorted(x2, all_x, side='right') / len(sample2)\n",
    "    \n",
    "    # Compute the KS distance\n",
    "    ks_distance = np.max(np.abs(ecdf1 - ecdf2))\n",
    "    return ks_distance\n",
    "\n",
    "def ale_categorical_ordering(X, features, \n",
    "                             other_categorical_features, other_categorical_features_separated, \n",
    "                             specific_categories=None):\n",
    "    if specific_categories is not None:\n",
    "        distance_matrix = np.zeros((3,3))\n",
    "        for i in range(3):\n",
    "            for j in range(i,3):\n",
    "                if i == j:\n",
    "                    distance_matrix[i,j] = 0\n",
    "                else:\n",
    "                    distance = 0\n",
    "                    maski = (np.sum(X[:,features]*np.array([1,2]),axis=1)==(i+1))\n",
    "                    maskj = (np.sum(X[:,features]*np.array([1,2]),axis=1)==(j+1))\n",
    "                    Xi = (X.copy())[maski]\n",
    "                    Xj = (X.copy())[maskj]\n",
    "                    for k in range(Xj.shape[1]):\n",
    "                        if k in other_categorical_features:\n",
    "                            continue\n",
    "                        x_curri = Xi[:,k]\n",
    "                        x_currj = Xj[:,k]\n",
    "                        distance += KS_dist(x_curri,x_currj)\n",
    "                    for cat_fts_other in other_categorical_features_separated:\n",
    "                        probsi = np.zeros(len(cat_fts_other))\n",
    "                        probsj = np.zeros(len(cat_fts_other))\n",
    "                        tempi = Xi[:,cat_fts_other]\n",
    "                        tempj = Xj[:,cat_fts_other]\n",
    "                        for q in range(len(cat_fts_other)):\n",
    "                            probsi[q] = np.sum(tempi[:,q]==1)\n",
    "                            probsj[q] = np.sum(tempj[:,q]==1)\n",
    "                        probsi = probsi/np.sum(probsi)\n",
    "                        probsj = probsj/np.sum(probsj)\n",
    "                        distance += np.sum(np.abs(probsi-probsj))\n",
    "                    distance_matrix[i,j] = distance_matrix[j,i] = distance\n",
    "\n",
    "    if specific_categories is None:\n",
    "        N_cat = len(features)\n",
    "        distance_matrix = np.zeros((N_cat,N_cat))\n",
    "        for i in range(N_cat):\n",
    "            maski = X[:,features[i]]==1\n",
    "            Xi = X.copy()\n",
    "            Xi = Xi[maski,:]\n",
    "            for j in range(i,N_cat):\n",
    "                if j == i:\n",
    "                    distance_matrix[i,j] = 0\n",
    "                else:\n",
    "                    distance = 0\n",
    "                    mask2 = X[:,features[j]]==1\n",
    "                    Xj = X.copy()\n",
    "                    Xj = Xj[mask2,:]\n",
    "                    for k in range(Xj.shape[1]):\n",
    "                        if k in other_categorical_features:\n",
    "                            continue\n",
    "                        x_curri = Xi[:,k]\n",
    "                        x_currj = Xj[:,k]\n",
    "                        distance += KS_dist(x_curri,x_currj)\n",
    "                    for cat_fts_other in other_categorical_features_separated:\n",
    "                        if len(cat_fts_other) == 2:\n",
    "                            probsi = np.zeros(3)\n",
    "                            probsj = np.zeros(3)\n",
    "                            tempi = Xi[:,cat_fts_other]\n",
    "                            tempj = Xj[:,cat_fts_other]\n",
    "                            for q in range(3):\n",
    "                                if q == 0:\n",
    "                                    probsi[q] = np.sum((tempi[:,0]==1)&(tempi[:,1]==1))\n",
    "                                    probsj[q] = np.sum((tempj[:,0]==1)&(tempj[:,1]==1))\n",
    "                                else:\n",
    "                                    probsi[q] = np.sum(tempi[:,q-1]==1)-probsi[0]\n",
    "                                    probsj[q] = np.sum(tempj[:,q-1]==1)-probsj[0]\n",
    "                        else:\n",
    "                            probsi = np.zeros(len(cat_fts_other))\n",
    "                            probsj = np.zeros(len(cat_fts_other))\n",
    "                            tempi = Xi[:,cat_fts_other]\n",
    "                            tempj = Xj[:,cat_fts_other]\n",
    "                            for q in range(len(cat_fts_other)):\n",
    "                                probsi[q] = np.sum(tempi[:,q]==1)\n",
    "                                probsj[q] = np.sum(tempj[:,q]==1)\n",
    "                        probsi = probsi/np.sum(probsi)\n",
    "                        probsj = probsj/np.sum(probsj)\n",
    "                        distance += np.sum(np.abs(probsi-probsj))\n",
    "                    distance_matrix[i,j] = distance_matrix[j,i] = distance\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "def ale_categorical(model, X, features, order, specific_categories=None):\n",
    "    \"\"\"\n",
    "    Calculate ALE for a categorical feature\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fitted model with predict method\n",
    "    X : pandas DataFrame, input data\n",
    "    feature_name : str, name of categorical feature\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ale_values : dict, ALE values for each category\n",
    "    \"\"\"\n",
    "    specific_cat_is_None = False\n",
    "    if specific_categories is None:\n",
    "        specific_categories = np.eye(len(features))\n",
    "        specific_cat_is_None = True\n",
    "    \n",
    "    specific_categories = specific_categories[order]\n",
    "    ale_values = np.zeros(len(specific_categories))\n",
    "    probs = np.zeros(len(specific_categories))\n",
    "    \n",
    "    # Calculate ALE for each category\n",
    "    for i in range(-1,len(specific_categories)-1):\n",
    "        if specific_cat_is_None:\n",
    "            mask_curr = np.sum(X[:,features][:,i+1]==1)\n",
    "        else:\n",
    "            mask_curr = (np.sum(np.array(X[:,features])*np.array([1,2]),axis=1)==np.sum(specific_categories[i+1]*np.array([1,2])))\n",
    "        p_curr = np.sum(mask_curr)/X.shape[0]\n",
    "\n",
    "        # Create a modified dataset with all values set to the current category\n",
    "        X_mod = (X.copy())[mask_curr]\n",
    "        N = X_mod.shape[0]\n",
    "        if i == -1:\n",
    "            X_mod[:,features] = np.zeros_like(X_mod[:,features])\n",
    "        else:\n",
    "            X_mod[:,features] = np.repeat(specific_categories[i].reshape(1,-1),repeats=N,axis=0)\n",
    "        y_left = model.predict(X_mod)\n",
    "\n",
    "        X_mod[:,features] = np.repeat(specific_categories[i+1].reshape(1,-1),repeats=N,axis=0)\n",
    "        y_right = model.predict(X_mod)\n",
    "\n",
    "        # Compute local effects and store mean difference\n",
    "        ale_values[i+1] = np.mean((y_right - y_left))\n",
    "        probs[i+1] = p_curr\n",
    "    \n",
    "    ale_values = np.cumsum(ale_values)\n",
    "    ale_values -= np.sum(ale_values*probs)\n",
    "    \n",
    "    return ale_values, probs\n",
    "\n",
    "def compute_ale_continuous(model, X, feature, grid_size=20, use_quantile=False):\n",
    "    \"\"\"\n",
    "    Compute Aggregated Local Effects (ALE) for a given feature.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained predictive model with a `.predict()` method.\n",
    "        X (DataFrame): Feature dataset.\n",
    "        feature (str): Feature for which to compute ALE.\n",
    "        grid_size (int): Number of bins (grid intervals) to partition the feature range.\n",
    "    \n",
    "    Returns:\n",
    "        (bin_centers, ale_values): X-axis (feature values) and corresponding ALE values.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    feature_values = X[:,feature]\n",
    "\n",
    "    if use_quantile:\n",
    "        quantiles = np.linspace(0, 1, grid_size+1)\n",
    "        bin_edges = np.quantile(feature_values, quantiles)\n",
    "    else:\n",
    "        bin_edges = np.linspace(feature_values.min(), feature_values.max(), grid_size + 1)\n",
    "    \n",
    "    # Compute bin midpoints\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Assign samples to bins\n",
    "    bin_indices = np.digitize(feature_values, bin_edges) - 1  # Bin index for each sample\n",
    "    \n",
    "    ale_values = np.zeros(grid_size)  # ALE values for each bin\n",
    "    probs = np.zeros(grid_size)\n",
    "    \n",
    "    for i in range(1, grid_size):  # Skip first bin since we can't compute difference\n",
    "        # Select samples in bin i and i-1\n",
    "        X_copy = X.copy()\n",
    "        mask = (bin_indices == i)\n",
    "        mask = mask.reshape(-1)\n",
    "        # print(mask.shape)\n",
    "        p_curr = np.sum(mask)/mask.shape[0]\n",
    "        \n",
    "        if mask.sum() == 0:  # Skip empty bins\n",
    "            continue\n",
    "\n",
    "        # Compute model prediction with feature values shifted to left and right bin edges\n",
    "        X_copy[:,feature] = bin_edges[i]  # Left boundary\n",
    "        y_left = model.predict(X_copy)\n",
    "\n",
    "        X_copy[:,feature] = bin_edges[i + 1]  # Right boundary\n",
    "        y_right = model.predict(X_copy)\n",
    "\n",
    "        # Compute local effects and store mean difference\n",
    "        ale_values[i] = np.mean((y_right[mask] - y_left[mask]))\n",
    "        probs[i] = p_curr\n",
    "\n",
    "    # Convert local effects to cumulative sum (ALE values)\n",
    "    ale_values = np.cumsum(ale_values)\n",
    "    \n",
    "    # Center ALE by subtracting mean\n",
    "    ale_values -= np.sum(ale_values*probs)\n",
    "    \n",
    "    return bin_centers, ale_values, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location_indices = [65,66] #Location\n",
    "Media_indices = list(range(56,65))\n",
    "Prod_indices = list(range(71,109))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_indices_location_tot = []\n",
    "ordered_indices_media_tot = []\n",
    "ordered_indices_prod_tot = []\n",
    "\n",
    "for i in range(10):\n",
    "    (X_train, Y_train, X_test, Y_test) = torch.load('Data/dataset_'+str(0))\n",
    "    X_to_inspect = np.concatenate((X_train,X_test),axis=0)\n",
    "\n",
    "    #Location\n",
    "    categorical_inspect = np.array(Location_indices)\n",
    "    other_categorical_features = np.array(Media_indices+Prod_indices)\n",
    "    other_categorical_features_separated = [np.array(list(range(56,65))), np.array(list(range(71,109)))]\n",
    "    distance_matrix_location = ale_categorical_ordering(np.array(X_to_inspect), categorical_inspect, \n",
    "                                other_categorical_features, other_categorical_features_separated, \n",
    "                                specific_categories=[[0,1]])\n",
    "    \n",
    "    #Media\n",
    "    categorical_inspect = np.array(Media_indices)\n",
    "    other_categorical_features = np.array(Location_indices+Prod_indices)\n",
    "    other_categorical_features_separated = [np.array(Location_indices), np.array(Prod_indices)]\n",
    "    distance_matrix_media = ale_categorical_ordering(np.array(X_to_inspect), categorical_inspect, \n",
    "                                other_categorical_features, other_categorical_features_separated, \n",
    "                                specific_categories=None)\n",
    "    \n",
    "    #Prod\n",
    "    categorical_inspect = np.array(Prod_indices)\n",
    "    other_categorical_features = np.array(Location_indices+Media_indices)\n",
    "    other_categorical_features_separated = [np.array(Location_indices), np.array(Media_indices)]\n",
    "    distance_matrix_prod = ale_categorical_ordering(np.array(X_to_inspect), categorical_inspect, \n",
    "                                other_categorical_features, other_categorical_features_separated, \n",
    "                                specific_categories=None)\n",
    "    \n",
    "    mds_location = MDS(n_components=1, dissimilarity='precomputed', random_state=42)\n",
    "    positions_location = mds_location.fit_transform(distance_matrix_location).flatten()\n",
    "    ordered_indices_location = np.argsort(positions_location)\n",
    "    ordered_indices_location_tot.append(ordered_indices_location)\n",
    "\n",
    "    mds_media = MDS(n_components=1, dissimilarity='precomputed', random_state=42)\n",
    "    positions_media = mds_media.fit_transform(distance_matrix_media).flatten()\n",
    "    ordered_indices_media = np.argsort(positions_media)\n",
    "    ordered_indices_media_tot.append(ordered_indices_media)\n",
    "\n",
    "    mds_prod = MDS(n_components=1, dissimilarity='precomputed', random_state=42)\n",
    "    positions_prod = mds_prod.fit_transform(distance_matrix_prod).flatten()\n",
    "    ordered_indices_prod = np.argsort(positions_prod)\n",
    "    ordered_indices_prod_tot.append(ordered_indices_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaze_Types = ['Ad_Gaze', 'Brand_Gaze', 'Brand_Share']\n",
    "ALE_values_tot_continuous = []\n",
    "ALE_values_tot_discrete = []\n",
    "predicted_values_tot = []\n",
    "for gtype in Gaze_Types:\n",
    "    shap_values = 0\n",
    "    predict_vals = 0\n",
    "    curr = np.zeros(111)\n",
    "    curr_cat = np.zeros(3)\n",
    "    for i in range(10):\n",
    "        if (i+1)%5 == 0:\n",
    "            print('i: ', i)\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        model_path = os.path.join(parent_dir, f'src/{gtype}_Model/10_models/Model_{i+1}.json')\n",
    "        xgb_model.load_model(model_path)\n",
    "        (X_train, Y_train, X_test, Y_test) = torch.load('Data/dataset_'+str(0))\n",
    "        for j in range(111):\n",
    "            if j in Location_indices+Media_indices+Prod_indices:\n",
    "                continue\n",
    "            _ , ale_vals, probs = compute_ale_continuous(xgb_model, np.concatenate((X_train,X_test),axis=0), j, grid_size=100, use_quantile=False)\n",
    "            curr[j] += np.sum(np.abs(ale_vals)*probs)/10#ale_vals[-1]/10\n",
    "        ale_location, probs_loc = ale_categorical(xgb_model, np.concatenate((X_train,X_test),axis=0), Location_indices, ordered_indices_location_tot[i], specific_categories=np.array([[1,0],[0,1],[1,1]]))\n",
    "        ale_media, probs_media = ale_categorical(xgb_model, np.concatenate((X_train,X_test),axis=0), Media_indices, ordered_indices_media_tot[i], specific_categories=None)\n",
    "        ale_prod, probs_prod = ale_categorical(xgb_model, np.concatenate((X_train,X_test),axis=0), Prod_indices, ordered_indices_prod_tot[i], specific_categories=None)\n",
    "        curr_cat += np.array([np.sum(np.abs(ale_location)*probs_loc),\n",
    "                              np.sum(np.abs(ale_media)*probs_media),\n",
    "                              np.sum(np.abs(ale_prod)*probs_prod)])/10 #np.array([ale_location[-1],ale_media[-1],ale_prod[-1]])/10\n",
    "    ALE_values_tot_discrete.append(curr_cat)\n",
    "    ALE_values_tot_continuous.append(curr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    ALE_values_tot_continuous[i][np.array(Location_indices)] = ALE_values_tot_discrete[i][0]/len(Location_indices)\n",
    "    ALE_values_tot_continuous[i][np.array(Media_indices)] = ALE_values_tot_discrete[i][1]/len(Media_indices)\n",
    "    ALE_values_tot_continuous[i][np.array(Prod_indices)] = ALE_values_tot_discrete[i][2]/len(Prod_indices)\n",
    "    ALE_values_tot_continuous[i] = ALE_values_tot_continuous[i].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ALE_values_tot_continuous, 'ALE_interpretation_simple_grid_abs_of_mean_avg_on_x_centered_3200_ads.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XGBGaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
